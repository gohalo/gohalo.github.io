<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>线性回归 基本介绍 | GoHalo</title><link rel=apple-touch-icon sizes=180x180 href=https://gohalo.github.io/favicon/apple-touch-icon.png><link rel=icon href=https://gohalo.github.io/favicon/favicon.ico sizes=any><link rel=icon type=image/png sizes=32x32 href=https://gohalo.github.io/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://gohalo.github.io/favicon/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://gohalo.github.io/favicon/site.webmanifest><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=google-site-verification content="p7jJ5d3kF9yxRhpIo5GgHXAZ1ATKVyZhV2kf6mEGOv0"><meta name=description content="回归分析 (Regression Analysis) 是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，线性回归 (Linear Regression) 也是最基本的。
这里介绍其概念、公式推导以及基本的实现。
"><link rel=stylesheet href=https://gohalo.github.io/font-awesome/css/font-awesome.min.css><link rel=stylesheet href=/css/syntax.min.c70103877c799b924f50023b6b01eca010d7e2808885a74f9ea662cc47379ae1.css integrity="sha256-xwEDh3x5m5JPUAI7awHsoBDX4oCIhadPnqZizEc3muE=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.c4814ac9dc5fab259f313a787ded4f8e.css integrity="md5-xIFKydxfqyWfMTp4fe1Pjg==" crossorigin=anonymous><link rel=stylesheet href=https://gohalo.github.io/katex/katex.min.css><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><style type=text/css>.main p{text-indent:2em}.main li p{text-indent:0}</style><noscript><style>img.lazyload{display:none}</style></noscript></head><body><div class=sticky-top><div class=header-bar></div><nav class="navbar navbar-expand-lg navbar-light bg-light"><div class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=/cn aria-label=GoHalo>GoHalo</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>项目</a><ul class=dropdown-menu aria-labelledby=navbarDropdown><li><a class=dropdown-item href=/cn/project/bastion/>Bastion</a></li><li><a class="dropdown-item disabled" href=/cn/project/bootserver/>BootServer</a></li></ul></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/>博客</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/archives/>归档</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/tags/>标签</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/slide/>幻灯片</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/docs/>文档</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><div class=dropdown><button class="btn dropdown-toggle" id=header-languages data-bs-toggle=dropdown aria-expanded=false data-bs-display=static>
中文</button><ul class="dropdown-menu dropdown-menu-lg-end me-lg-2 shadow rounded border-0" aria-labelledby=header-languages><li><a class=dropdown-item rel=alternate href=https://gohalo.github.io/en hreflang=en lang=en>English</a></li></ul></div><ul class="nav flex-column flex-lg-row"><li class=nav-item><a class="nav-link social-link" href=/cn/about><i class="fa fa-user" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link social-link" href=https://github.com/gohalo title=GitHub><i class="fa fa-github-alt" aria-hidden=true></i></a></li><li class=nav-item><button id=mode class="btn nav-link social-link" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><i class="fa fa-star" aria-hidden=true></i></span>
<span class=toggle-light><i class="fa fa-cog" aria-hidden=true></i></span></button></li></ul></div></div></nav></div><div class="main container-xxl" role=document><div class="blog row"><div class="col-md-12 col-xl-9 mt-4"><div class=header><h1>线性回归 基本介绍</h1><div class="meta mb-3"><i class="fa fa-calendar" aria-hidden=true></i>
<span class=mx-2>2018-02-01</span>
<i class="fa fa-tags" aria-hidden=true></i>
<a class=text-body href=https://gohalo.github.io/cn/tags/ai/ role=button>ai</a></div></div><hr><div class=content><p>回归分析 (Regression Analysis) 是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，线性回归 (Linear Regression) 也是最基本的。</p><p>这里介绍其概念、公式推导以及基本的实现。</p><a class=anchor id=简介></a><h1>简介 <a href=#%e7%ae%80%e4%bb%8b aria-hidden=true>#</a></h1><p>线性回归是机器学习中一个最基本的类型，试图从数据集中找到一个线性模型，这个模型可以准确反映出输入 $X_i$ 和输出 $Y_i$ 的对应关系。</p><p>假设每个数据样本有 $d$ 维，对应的线性函数如下。</p><p>$$f(x) = w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + b$$</p><p>首先从一个简单的示例介绍。</p><a class=anchor id=线性回归></a><h2>线性回归 <a href=#%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92 aria-hidden=true>#</a></h2><p>简单来说，现在有一批的数据，示例如下。</p><p><img alt="linear regression example data" src=images/linear-regression-example-data.png class="mx-auto d-block"></p><p>可以通过如下代码生成如上的测试数据。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>pylab</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>10.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>x</span> <span class=o>-</span> <span class=mi>5</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>scale</span><span class=o>=</span><span class=mf>2.0</span><span class=o>**</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pylab</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pylab</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;x&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>15</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pylab</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;y&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>15</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pylab</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;What our data looks like&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>12</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pylab</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p>我们希望通过这些数据来拟合出一条直线。</p><p>$$y=w_1 x + w_0$$</p><p>也就是要确定 $w_1$ 和 $w_0$ 的值，一般来说，最常用的计算方法是获取这些数据最小的 Root Mean Squared Error, RMSE ，也即。</p><p>$$w_1, w_0=arg \ min_{w_1, w_0} \sum_{i=1}^{N}(y_i - (w_1 x_i + w_0))^2$$</p><p>直接使用现成的库，后面在介绍直接使用 numpy 的方式。</p><a class=anchor id=普通最小二乘法></a><h2>普通最小二乘法 <a href=#%e6%99%ae%e9%80%9a%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98%e6%b3%95 aria-hidden=true>#</a></h2><p>如上所述，也就是找到一条参数对应的曲线，使得所有数据具有最小均方根误差 (RMSE)，可以直接使用 <code>Scikit-Learn</code> 中的 <code>LinearRegression</code> 模块，完整的代码如下。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>pylab</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LinearRegression</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>10.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>x</span> <span class=o>-</span> <span class=mi>5</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>scale</span><span class=o>=</span><span class=mf>2.0</span><span class=o>**</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>lr</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>lr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pylab</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pylab</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>lr</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=n>x</span> <span class=o>+</span> <span class=n>lr</span><span class=o>.</span><span class=n>intercept_</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;r&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pylab</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;x&#39;</span><span class=p>,</span><span class=n>fontsize</span><span class=o>=</span><span class=mi>18</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pylab</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;y&#39;</span><span class=p>,</span><span class=n>fontsize</span><span class=o>=</span><span class=mi>18</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pylab</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p>如上处理时，sklearn 要求输入的特征必须是二维数组的类型，但目前只有 1 个特征，需要用 <code>reshape()</code> 转行成二维数组的类型。</p><p>最终得到的直线如下所示。</p><p><img alt="linear regression example" src=images/linear-regression-scikit-learn-example.png class="mx-auto d-block"></p><p>实际上这里缺少一个度量，也就是 置信界限 (confidence bounds)，当某个点的数据比较多时，认为这个范围内可信度大，而当数据较少时则不确定性增加。</p><p>这一问题涉及到了后面介绍的贝叶斯线性回归。</p><a class=anchor id=公式推导></a><h1>公式推导 <a href=#%e5%85%ac%e5%bc%8f%e6%8e%a8%e5%af%bc aria-hidden=true>#</a></h1><a class=anchor id=一元推导></a><h2>一元推导 <a href=#%e4%b8%80%e5%85%83%e6%8e%a8%e5%af%bc aria-hidden=true>#</a></h2><p>假设在一个样本集合 $D$ 中，有 $n$ 个样本 ${(x_1, y_1), (x_2, y_2), \cdots , (x_n, y_n)}$ 。</p><a class=anchor id=代价函数></a><h3>代价函数 <a href=#%e4%bb%a3%e4%bb%b7%e5%87%bd%e6%95%b0 aria-hidden=true>#</a></h3><p>一般通过均方误差 (欧式距离) 对结果进行评估，也就是代价函数 (Cost Function) ，表示为如下。</p><p>$$(w^<em>, b^</em>)=arg \ min_{(w,b)} \sum_{i=1}^{m} (f(x_i) - y_i)^2$$</p><p>$arg \ min$ 是指后面表达式值最小时，对应 $(w, b)$ 的取值，实际上就是最小二乘法。接着就是要看如何求解参数，使得代价函数最小。</p><p>$$E(w, b) = \sum_{i=1}^m(y_i - wx_i - b)^2$$</p><p>其中，函数 $E$ 是关于参数 $(w, b)$ 的凸函数，那么这也就变成了一个凸优化问题，其中 $w$ 和 $b$ 是未知参数，使其对应的偏导为 $0$ 后，就可以得到最优解。</p><a class=anchor id=公式推导-1></a><h3>公式推导 <a href=#%e5%85%ac%e5%bc%8f%e6%8e%a8%e5%af%bc-1 aria-hidden=true>#</a></h3><p>首先分别对 $w$ 和 $b$ 求偏导，注意，这里使用的是一元。</p><p>$$
\begin{align}
\frac{\partial{E(w,b)}}{\partial{w}}&=\frac{\partial{\sum_{i=1}^m(y_i-wx_i-b)^2}}{\partial{w}} \
&=\frac{\partial{\sum_{i=1}^m((y_i-b)^2+w^2x_i^2-2w(y_i-b)x_i)}}{\partial{w}} \
&= \sum_{i=1}^m(2wx_i^2-2(y_i-b)x_i) \
&= 2(w\sum_{i=1}^m{x_i^2}-\sum_{i=1}^m(y_i-b)x_i)\</p><p>\frac{\partial{E(w,b)}}{\partial{b}}&=\frac{\partial{\sum_{i=1}^m(y_i-wx_i-b)^2}}{\partial{b}} \
&=\frac{\partial{\sum_{i=1}^m((y_i-wx_i)^2+b^2-2b(y_i-wx_i))}}{\partial{b}} \
&= \sum_{i=1}^m(2b-2(y_i-wx_i)) \
&= 2(mb-\sum_{i=1}^m{(y_i-wx_i)})
\end{align}
$$</p><p>然后令偏导 (也就是上述的结果) 为 $0$ 最终求解出结果。</p><p>$$
\begin{align}
w&=\frac{m\sum {x_i y_i}-\sum {x_i}\sum {y_i}}{m\sum {x_i^2}-(\sum x_i)^2} \
b&=\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)=\frac{\sum {x_i^2} \sum{ y_i}-\sum {x_i}\sum {x_i y_i}}{m\sum {x_i^2}-(\sum x_i)^2}
\end{align}
$$</p><a class=anchor id=高维推导></a><h2>高维推导 <a href=#%e9%ab%98%e7%bb%b4%e6%8e%a8%e5%af%bc aria-hidden=true>#</a></h2><p>假设在一个样本集合 $D$ 中，有 $n$ 个样本 ${(X_1, Y_1), (X_2, Y_2), \cdots , (X_n, Y_n)}$ ，其中每个样本由 $d$ 个属性，可以表示为输入特征向量为 $X = (x_1, x_2, \cdots , x_d)^T$ ，对应模型的预测输出 $f(x)$ 可以表示为输入 $X$ 的线性函数，也就是。</p><p>$$f(x) = w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + b$$</p><p>对应的矩阵形式为。</p><p>$$Y=f(x)=w^T x + b$$</p><p>其中 $x$ 和 $w$ 均为 $d \times 1$ 的列向量，其中 $w$ 代表权重 (Weight) 的意思。接着将 $w$ 和 $b$ 合并为 $\theta$ 。</p><a class=anchor id=梯度下降></a><h1>梯度下降 <a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d aria-hidden=true>#</a></h1><p>上面讨论的都是解析解，也就是直接通过公式进行计算，接着讨论的是通过梯度下降进行求解。</p></div></div><div class="sidebar col-xl-3 mt-2"><div id=toc class=position-fixed><nav id=TableOfContents><ul><li><a href=#简介>简介</a><ul><li><a href=#线性回归>线性回归</a></li><li><a href=#普通最小二乘法>普通最小二乘法</a></li></ul></li><li><a href=#公式推导>公式推导</a><ul><li><a href=#一元推导>一元推导</a></li><li><a href=#高维推导>高维推导</a></li></ul></li><li><a href=#梯度下降>梯度下降</a></li></ul></nav></div></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class=text-center>Built by GoHalo, generated with <a class=text-muted href=https://gohugo.io>Hugo</a>, and hosted on GitHub Pages</div></div><div class=row><div class=text-center>Copyright © 2013-2025 GoHalo. All Rights Reserved.</div></div></div></footer><script src=https://gohalo.github.io/bootstrap/js/bootstrap.bundle.min.js></script>
<script src=https://gohalo.github.io/katex/katex.min.js></script>
<script src=https://gohalo.github.io/katex/contrib/auto-render.min.js onload=renderMathInElement(document.body) defer></script>
<script src=/main.b9cbcb174709877512d64e24f297f66a40c8d91c9a81128cb04bdd7b10247df8.js integrity="sha256-ucvLF0cJh3US1k4k8pf2akDI2RyagRKMsEvdexAkffg=" crossorigin=anonymous></script>
<a href=# class="btn btn-light btn-lg backtop" title=返回顶部><i class="fa fa-angle-double-up" aria-hidden=true></i></a></body></html>