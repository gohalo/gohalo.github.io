<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Hudi 基本介绍 | GoHalo</title><link rel=apple-touch-icon sizes=180x180 href=https://gohalo.github.io/favicon/apple-touch-icon.png><link rel=icon href=https://gohalo.github.io/favicon/favicon.ico sizes=any><link rel=icon type=image/png sizes=32x32 href=https://gohalo.github.io/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://gohalo.github.io/favicon/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://gohalo.github.io/favicon/site.webmanifest><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=google-site-verification content="p7jJ5d3kF9yxRhpIo5GgHXAZ1ATKVyZhV2kf6mEGOv0"><meta name=description content="Hadoop Updates and Incrementals, Hudi 是一个 Uber 开源的 Data Lakes 的解决方案。
"><link rel=stylesheet href=https://gohalo.github.io/font-awesome/css/font-awesome.min.css><link rel=stylesheet href=/css/syntax.min.c70103877c799b924f50023b6b01eca010d7e2808885a74f9ea662cc47379ae1.css integrity="sha256-xwEDh3x5m5JPUAI7awHsoBDX4oCIhadPnqZizEc3muE=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.c4814ac9dc5fab259f313a787ded4f8e.css integrity="md5-xIFKydxfqyWfMTp4fe1Pjg==" crossorigin=anonymous><style type=text/css>.main p{text-indent:2em}.main li p{text-indent:0}</style><noscript><style>img.lazyload{display:none}</style></noscript></head><body><div class=sticky-top><div class=header-bar></div><nav class="navbar navbar-expand-lg navbar-light bg-light"><div class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=/cn aria-label=GoHalo>GoHalo</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>项目</a><ul class=dropdown-menu aria-labelledby=navbarDropdown><li><a class=dropdown-item href=/cn/project/bastion/>Bastion</a></li><li><a class="dropdown-item disabled" href=/cn/project/bootserver/>BootServer</a></li></ul></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/>博客</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/archives/>归档</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/tags/>标签</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/slide/>幻灯片</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/docs/>文档</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><div class=dropdown><button class="btn dropdown-toggle" id=header-languages data-bs-toggle=dropdown aria-expanded=false data-bs-display=static>
中文</button><ul class="dropdown-menu dropdown-menu-lg-end me-lg-2 shadow rounded border-0" aria-labelledby=header-languages><li><a class=dropdown-item rel=alternate href=https://gohalo.github.io/en hreflang=en lang=en>English</a></li></ul></div><ul class="nav flex-column flex-lg-row"><li class=nav-item><a class="nav-link social-link" href=/cn/about><i class="fa fa-user" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link social-link" href=https://github.com/gohalo title=GitHub><i class="fa fa-github-alt" aria-hidden=true></i></a></li><li class=nav-item><button id=mode class="btn nav-link social-link" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><i class="fa fa-star" aria-hidden=true></i></span>
<span class=toggle-light><i class="fa fa-cog" aria-hidden=true></i></span></button></li></ul></div></div></nav></div><div class="main container-xxl" role=document><div class="blog row"><div class="col-md-12 col-xl-9 mt-4"><div class=header><h1>Hudi 基本介绍</h1><div class="meta mb-3"><i class="fa fa-calendar" aria-hidden=true></i>
<span class=mx-2>2022-09-21</span>
<i class="fa fa-tags" aria-hidden=true></i>
<a class=text-body href=https://gohalo.github.io/cn/tags/warehouse/ role=button>warehouse</a>
<a class=text-body href=https://gohalo.github.io/cn/tags/hudi/ role=button>hudi</a></div></div><hr><div class=content><p>Hadoop Updates and Incrementals, Hudi 是一个 Uber 开源的 Data Lakes 的解决方案。</p><a class=anchor id=基本概念></a><h1>基本概念 <a href=#%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5 aria-hidden=true>#</a></h1><p>Hudi 提供了记录级的更新，每条记录都由 <code>HoodieKey</code> 唯一标识，通常是由分区路径(分区+FileID)和记录键组成，记录键在分区内保持唯一，记录键则通过 <code>KeyGenerator</code> 生成，支持多种形式以适配不同场景。</p><p>主键在 Hudi 中是一等公民，根据主键去重，同时利用 <code>preCombineField</code> 判断是否替换旧数据，只要比较大就替换，一般是 <code>ts</code> 时间戳，如果不设置则默认每次都用最新数据替换。</p><a class=anchor id=文件布局></a><h2>文件布局 <a href=#%e6%96%87%e4%bb%b6%e5%b8%83%e5%b1%80 aria-hidden=true>#</a></h2><p>在 <a href=https://hudi.apache.org/tech-specs>Apache Hudi Technical Specification</a> 包含了不同的版本，文件在存储介质上的分布，Hudi 会严格管理文件的命名、存放位置、大小等，会选择合适的实际新建、合并以及分裂数据文件。</p><p>对于 <code>MOR</code> 来说，文件切片 <code>File Slice</code> 由基本文件和多个增量日志文件组成，而 <code>COW</code> 只存在基础数据文件，不含日志文件。其中 <code>File Slice</code> 与某个提交关联，是文件组织系统中的最小管理单元。</p><p>相同 <code>FileID</code> 的文件属于同一个 <code>File Group</code>，通常会有多个不同的版本 (<code>InstantTime</code>) 的数据文件或者再加上日志文件的组合，当某个 <code>File Group</code> 超过阈值之后会创建新的 <code>File Group</code>，可以通过分区路径和文件 <code>ID</code> 唯一标识。</p><a class=anchor id=表类型></a><h2>表类型 <a href=#%e8%a1%a8%e7%b1%bb%e5%9e%8b aria-hidden=true>#</a></h2><p>所有的湖格式实际上都为了解决新鲜度(准确性)、成本和查询延时(实时性)这个三角问题，要保证其中一项就需要牺牲另外两项。例如，为了提升新鲜度，引入了 MOR 包，就导致的了成本和查询延迟增加，主要是增量日志、查询时的 Merge 操作。</p><p>Hudi 支持 <code>COW</code> 和 <code>MOR</code> 两种格式，分别用于不同的场景：</p><ul><li><code>Copy On Write, COW</code> 适用于离线批量更新场景，更新数据读取旧的 <code>BaseFile</code> 合并更新数据，生成新的 <code>BaseFile</code>，读取时直接访问最新的 <code>BaseFile</code> 即可，但是会导致写入放大。</li><li><code>Merge On Read, MOR</code> 前者每次更新会刷新原数据，导致写入放大；后者则保存增量信息，每次读取会合并增量信息。</li></ul><a class=anchor id=示例></a><h1>示例 <a href=#%e7%a4%ba%e4%be%8b aria-hidden=true>#</a></h1><p>这里以 SparkSQL 为例进行操作。</p><a class=anchor id=简单介绍></a><h2>简单介绍 <a href=#%e7%ae%80%e5%8d%95%e4%bb%8b%e7%bb%8d aria-hidden=true>#</a></h2><p>注意，不同 Spark 版本对应的配置参数略有区别，详细可以参考官网 <a href=https://hudi.apache.org/docs/next/quick-start-guide>Spark Quick Start</a> 的介绍。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>----- 执行命令行时已经包含 hudi-sparkX.X-bundle_xxx.jar 路径
</span></span><span class=line><span class=cl>$ spark-sql --master yarn --deploy-mode client
</span></span><span class=line><span class=cl>----- 否则需要如下方式指定参数
</span></span><span class=line><span class=cl>$ spark-sql --master local[4] \
</span></span><span class=line><span class=cl>   --jars hudi-spark-bundle/target/hudi-spark3.3-bundle_2.12-0.13.1.jar \
</span></span><span class=line><span class=cl>   --jars spark/jars/spark-avro_2.12-341.jar \
</span></span><span class=line><span class=cl>   --conf &#39;spark.serializer=org.apache.spark.serializer.KryoSerializer&#39; \
</span></span><span class=line><span class=cl>   --conf &#39;spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension&#39;
</span></span></code></pre></div><p>除了在命令行参数中指定配置之外，还可以通过 <code>HUDI_CONF_DIR</code> 环境变量指定配置文件，同时增加一个 <code>hudi-default.conf</code> 的配置文件。接着可以创建表结构。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>spark-sql&gt; CREATE TABLE test_hudi_table(
</span></span><span class=line><span class=cl>  id INT,
</span></span><span class=line><span class=cl>  name STRING,
</span></span><span class=line><span class=cl>  price DOUBLE,
</span></span><span class=line><span class=cl>  ts LONG,
</span></span><span class=line><span class=cl>  dt STRING
</span></span><span class=line><span class=cl>)
</span></span><span class=line><span class=cl>USING hudi
</span></span><span class=line><span class=cl>PARTITIONED BY(dt)
</span></span><span class=line><span class=cl>OPTIONS (
</span></span><span class=line><span class=cl>  type = &#39;mor&#39;,
</span></span><span class=line><span class=cl>  primaryKey = &#39;id&#39;,
</span></span><span class=line><span class=cl>  preCombineField = &#39;ts&#39;
</span></span><span class=line><span class=cl>) LOCATION &#39;/tmp/hudi_mor_table&#39;;
</span></span></code></pre></div><p>已经存在的表，可以通过如下方式创建。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>CREATE TABLE test_hudi_table
</span></span><span class=line><span class=cl>USING hudi
</span></span><span class=line><span class=cl>LOCATION &#39;/tmp/hudi_mor_table&#39;;
</span></span></code></pre></div><p>除此之外，还可以使用 Create Table As Select 创建表，对于 <code>LOCATION</code> 参数来说，默认一般是 HDFS 存储，也可以通过如下方式指定本地盘，或者显示指定 HDFS 存储。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>----- 在本地磁盘下创建
</span></span><span class=line><span class=cl>file:///tmp/hudi_mor_table
</span></span><span class=line><span class=cl>----- 指定HDFS存储
</span></span><span class=line><span class=cl>hdfs://hacluster/tmp/hudi_mor_table
</span></span><span class=line><span class=cl>----- 指定对象存储，以华为云为例
</span></span><span class=line><span class=cl>obs://starrocks/hudi/mor_table
</span></span></code></pre></div><p>建表语句的元数据信息会同步到 Hive MetaStore 中，此时，如果登录 Hive 查看元数据，可以看到如下内容。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>$ beeline
</span></span><span class=line><span class=cl>jdbc:hive2://xxx&gt; SHOW TABLES;
</span></span><span class=line><span class=cl>+---------------------+
</span></span><span class=line><span class=cl>|      tab_name       |
</span></span><span class=line><span class=cl>+---------------------+
</span></span><span class=line><span class=cl>| test_hudi_table     |
</span></span><span class=line><span class=cl>| test_hudi_table_ro  |
</span></span><span class=line><span class=cl>| test_hudi_table_rt  |
</span></span><span class=line><span class=cl>+---------------------+
</span></span><span class=line><span class=cl>3 rows selected (0.263 seconds)
</span></span></code></pre></div><p>MOR 表会自动生成 ro/rt 后缀的表，实际上就是 Read Optimized 和 Real Time 的缩写，</p><p>如下是常用的 SQL 语句。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>INSERT INTO test_hudi_table SELECT 1 AS id, &#39;hudi&#39; AS name, 10 AS price, 1000 AS ts, &#39;2021-05-05&#39; AS dt;
</span></span><span class=line><span class=cl>INSERT INTO test_hudi_table SELECT 2, &#39;spark&#39;, 30.0, 1000, &#39;2021-05-05&#39;;
</span></span><span class=line><span class=cl>INSERT INTO test_hudi_table VALUES(3, &#39;hello&#39;, 10.0, 1000, &#39;2021-05-05&#39;);
</span></span><span class=line><span class=cl>SELECT * FROM test_hudi_table WHERE price &gt; 20.0;
</span></span><span class=line><span class=cl>UPDATE test_hudi_table SET price = 20 WHERE id = 1;
</span></span><span class=line><span class=cl>DELETE FROM test_hudi_table WHERE id = 1;
</span></span><span class=line><span class=cl>DROP TABLE test_hudi_table_rt;
</span></span><span class=line><span class=cl>DROP TABLE test_hudi_table_ro;
</span></span><span class=line><span class=cl>DROP TABLE test_hudi_table;
</span></span><span class=line><span class=cl>SHOW TABLES;
</span></span></code></pre></div><p>当执行如下 SQL 时，由于 <code>id=3</code> 中的 <code>ts=1000>500</code> 实际不会替换，除非将 <code>ts</code> 替换为 <code>1500</code> 值。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>INSERT INTO test_hudi_table VALUES(3, &#39;not_change&#39;, 10.0, 500, &#39;2021-05-05&#39;);
</span></span></code></pre></div><p>另外，还可以通过如下方式通过动态分区或者指定分区的方式写入。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>INSERT INTO test_hudi_table VALUES(3, &#39;hello&#39;, 10.0, 1000, &#39;2021-05-05&#39;);
</span></span><span class=line><span class=cl>INSERT INTO test_hudi_table partition(dt = &#39;2021-05-05&#39;) VALUES(3, &#39;hello&#39;, 10.0, 1000);
</span></span></code></pre></div><p>测试发现一个很奇怪的现象：动态 partition 会生成 parquet 文件，静态则会生成 log 文件。</p><a class=anchor id=mergeinto></a><h2>MergeInto <a href=#mergeinto aria-hidden=true>#</a></h2><p>可以将两个表的数据按照规则进行合并。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>test_hudi_update</span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>id</span><span class=w> </span><span class=nb>INT</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>name</span><span class=w> </span><span class=n>STRING</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>price</span><span class=w> </span><span class=n>DOUBLE</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>ts</span><span class=w> </span><span class=n>LONG</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>dt</span><span class=w> </span><span class=n>STRING</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>USING</span><span class=w> </span><span class=n>hudi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>PARTITIONED</span><span class=w> </span><span class=k>BY</span><span class=p>(</span><span class=n>dt</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>OPTIONS</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=k>type</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;mor&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>primaryKey</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;id&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>preCombineField</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;ts&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span><span class=w> </span><span class=k>LOCATION</span><span class=w> </span><span class=s1>&#39;/tmp/hudi_mor_update&#39;</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>INSERT</span><span class=w> </span><span class=k>INTO</span><span class=w> </span><span class=n>test_hudi_update</span><span class=w> </span><span class=k>VALUES</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>(</span><span class=mi>3</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;hello&#39;</span><span class=p>,</span><span class=w> </span><span class=mi>50</span><span class=p>.</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>1000</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;2021-05-05&#39;</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>(</span><span class=mi>4</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;world&#39;</span><span class=p>,</span><span class=w> </span><span class=mi>10</span><span class=p>.</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>1000</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;2021-05-05&#39;</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>MERGE</span><span class=w> </span><span class=k>INTO</span><span class=w> </span><span class=n>test_hudi_table</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>target</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>USING</span><span class=w> </span><span class=n>test_hudi_update</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=k>source</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>ON</span><span class=w> </span><span class=n>target</span><span class=p>.</span><span class=n>id</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>source</span><span class=p>.</span><span class=n>id</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>WHEN</span><span class=w> </span><span class=n>MATCHED</span><span class=w> </span><span class=k>THEN</span><span class=w> </span><span class=k>UPDATE</span><span class=w> </span><span class=k>SET</span><span class=w> </span><span class=n>target</span><span class=p>.</span><span class=n>price</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>target</span><span class=p>.</span><span class=n>price</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=k>source</span><span class=p>.</span><span class=n>price</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>WHEN</span><span class=w> </span><span class=k>NOT</span><span class=w> </span><span class=n>MATCHED</span><span class=w> </span><span class=k>THEN</span><span class=w> </span><span class=k>INSERT</span><span class=w> </span><span class=o>*</span><span class=p>;</span><span class=w>
</span></span></span></code></pre></div><a class=anchor id=查询></a><h1>查询 <a href=#%e6%9f%a5%e8%af%a2 aria-hidden=true>#</a></h1><p>Hudi 支持如下几种查询类型：</p><ul><li><code>Snapshot</code> 默认，查询某个时间点最新数据，对于 <code>MOR</code> 会将 <code>Base</code> 和 <code>Log</code> 合并；而 <code>COW</code> 就是最新 <code>Base</code> 数据文件。</li><li><code>Read Optimized</code> 优化查询可能会读到旧的数据，对于 <code>MOR</code> 来说就是只读取 <code>Base</code> 数据。</li><li><code>Time Travel</code> 数据时按照时间线排序的，可以根据具体的时间进行查询。</li><li><code>Incremental</code> 获取两个提交范围内的数据。</li></ul><p>注意，只能查询到写入 Hudi 表的数据，也就是给定的 COMMIT/COMPACTION 之后的最新数据。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mysql data-lang=mysql><span class=line><span class=cl><span class=o>---</span><span class=c1>-- Snapshot Query
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>SELECT</span><span class=w> </span><span class=n>id</span><span class=p>,</span><span class=w> </span><span class=n>name</span><span class=p>,</span><span class=w> </span><span class=n>price</span><span class=p>,</span><span class=w> </span><span class=n>ts</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>test_hudi_table</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>---</span><span class=c1>-- Optimized Read Query 当配置了 hoodie.query.as.ro.table=true 之后会自动生成
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>SELECT</span><span class=w> </span><span class=n>id</span><span class=p>,</span><span class=w> </span><span class=n>name</span><span class=p>,</span><span class=w> </span><span class=n>price</span><span class=p>,</span><span class=w> </span><span class=n>ts</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>test_hudi_table_ro</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>---</span><span class=c1>-- TimeTravel 如果没有严格匹配上，则返回比指定时间略老的数据，需要 Spark 3.2+
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>SELECT</span><span class=w> </span><span class=n>id</span><span class=p>,</span><span class=w> </span><span class=n>name</span><span class=p>,</span><span class=w> </span><span class=n>price</span><span class=p>,</span><span class=w> </span><span class=n>ts</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>test_hudi_table</span><span class=w> </span><span class=kt>TIMESTAMP</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>OF</span><span class=w> </span><span class=s1>&#39;20240116113746358&#39;</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>SELECT</span><span class=w> </span><span class=n>id</span><span class=p>,</span><span class=w> </span><span class=n>name</span><span class=p>,</span><span class=w> </span><span class=n>price</span><span class=p>,</span><span class=w> </span><span class=n>ts</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>test_hudi_table</span><span class=w> </span><span class=kt>TIMESTAMP</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>OF</span><span class=w> </span><span class=s1>&#39;20240116113746&#39;</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>---</span><span class=c1>-- 查看直接空值即可
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>SET</span><span class=w> </span><span class=n>hoodie</span><span class=p>.</span><span class=n>datasource</span><span class=p>.</span><span class=n>query</span><span class=p>.</span><span class=n>type</span><span class=o>=</span><span class=n>incremental</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>hoodie</span><span class=p>.</span><span class=n>datasource</span><span class=p>.</span><span class=k>read</span><span class=p>.</span><span class=n>begin</span><span class=p>.</span><span class=n>instanttime</span><span class=o>=</span><span class=mi>20240419170531565</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>hoodie</span><span class=p>.</span><span class=n>datasource</span><span class=p>.</span><span class=k>read</span><span class=p>.</span><span class=n>end</span><span class=p>.</span><span class=n>instanttime</span><span class=o>=</span><span class=mi>202305160000</span><span class=w> </span><span class=c1># optional
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>---</span><span class=c1>-- 在 Hive 中可以根据提交时间排序
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>SELECT</span><span class=w> </span><span class=o>`</span><span class=n>_hoodie_commit_time</span><span class=o>`</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>t</span><span class=p>,</span><span class=w> </span><span class=n>id</span><span class=p>,</span><span class=w> </span><span class=n>name</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>test_hudi_table</span><span class=w> </span><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>t</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>---</span><span class=c1>-- Incremental 设置增量读取模式，同时配置起止时间戳
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=err>其中</span><span class=w> </span><span class=n>test</span><span class=w> </span><span class=err>为具体表明</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kt>set</span><span class=w> </span><span class=n>hoodie</span><span class=p>.</span><span class=n>test</span><span class=p>.</span><span class=n>consume</span><span class=p>.</span><span class=n>mode</span><span class=o>=</span><span class=n>INCREMENTAL</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kt>set</span><span class=w> </span><span class=n>hoodie</span><span class=p>.</span><span class=n>test</span><span class=p>.</span><span class=n>consume</span><span class=p>.</span><span class=n>start</span><span class=p>.</span><span class=kt>timestamp</span><span class=o>=</span><span class=mi>20240507190438</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kt>set</span><span class=w> </span><span class=n>hoodie</span><span class=p>.</span><span class=n>test</span><span class=p>.</span><span class=n>consume</span><span class=p>.</span><span class=n>end</span><span class=p>.</span><span class=kt>timestamp</span><span class=o>=</span><span class=mi>20240507300438</span><span class=p>;</span><span class=w> </span><span class=c1># optional
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>SELECT</span><span class=w> </span><span class=n>id</span><span class=p>,</span><span class=w> </span><span class=n>name</span><span class=p>,</span><span class=w> </span><span class=n>price</span><span class=p>,</span><span class=w> </span><span class=n>ts</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>test_hudi_table</span><span class=w> </span><span class=k>where</span><span class=w> </span><span class=o>`</span><span class=n>_hoodie_commit_time</span><span class=o>`&gt;</span><span class=s1>&#39;20240507190438&#39;</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>社区版本是不支持这种方式的，在</span><span class=w> </span><span class=mi>0</span><span class=p>.</span><span class=mi>15</span><span class=p>.</span><span class=mi>1</span><span class=w> </span><span class=err>中新增了</span><span class=w> </span><span class=n>TVF</span><span class=w> </span><span class=err>的方式。</span><span class=w>
</span></span></span></code></pre></div><p>其中 Spark 的版本可以通过 spark-shell 命令查看。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>call show_clustering(table =&gt; &#39;test_hudi_table&#39;);
</span></span><span class=line><span class=cl>call show_table_properties(table =&gt; &#39;test_hudi_table&#39;);
</span></span></code></pre></div><a class=anchor id=目录结构></a><h1>目录结构 <a href=#%e7%9b%ae%e5%bd%95%e7%bb%93%e6%9e%84 aria-hidden=true>#</a></h1><p>Hudi 数据集的目录结构和 Hive 非常相似，数据集根据分区打散到不同目录，分区字段以文件夹形式存在，文件夹包含该分区的所有文件，非分区则在直接在数据目录下保存。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>cow/.hoodis/
</span></span><span class=line><span class=cl>cow/.hoodis/.aux/
</span></span><span class=line><span class=cl>cow/.hoodis/.temp/
</span></span><span class=line><span class=cl>cow/.hoodis/20230523174532.commit            组成时间线的文件
</span></span><span class=line><span class=cl>cow/.hoodis/20230523174532.commit.inflight
</span></span><span class=line><span class=cl>cow/.hoodis/20230523174532.commit.requested
</span></span><span class=line><span class=cl>cow/.hoodis/archived/
</span></span><span class=line><span class=cl>cow/.hoodis/metadata/                        元数据目录，采用MOR结构，支持Payload不同 hoodie.metadata.enable
</span></span><span class=line><span class=cl>cow/.hoodis/hoodie.index.properties          相关的索引信息
</span></span><span class=line><span class=cl>cow/.hoodis/hoodie.properties                基础的表配置信息，例如表名、版本等
</span></span><span class=line><span class=cl>cow/americas/
</span></span><span class=line><span class=cl>cow/americas/.hoodis_partition_metadata      元数据，保存了提交的时间和分区深度信息，文本格式保存
</span></span><span class=line><span class=cl>cow/americas/.1419a23b-5ecd-46e6-b96c-07a395c8029a-0_20230828160921471.log.1_0-103-148
</span></span><span class=line><span class=cl>cow/americas/1419a23b-5ecd-46e6-b96c-07a395c8029a-0_0-81-116_20230828160921471.parquet
</span></span></code></pre></div><p>元数据目录为 <code>.hoodie</code>，其包含了版本管理 <code>Timeline</code>、归档信息 (过时的 <code>Instant</code>)、<code>Instant</code> 提交信息 (包括提交行为、时间戳、状态等) 等其它信息。</p><p>剩下目录是按分区实际存储的数据，分区键可以指定，基础数据文件采用 Parquet 格式保存，针对读进行了优化；还包含一个元数据日志文件 <code>.log*</code>，同样进行了写优化；还包括分区元数据 <code>.hoodie_partition_metadata</code> 文件。</p><p>注意，<code>COW</code> 只有数据文件，而 <code>MOR</code> 格式才会通过 <code>AVRO</code> 格式保存增量日志信息。</p><a class=anchor id=元数据></a><h2>元数据 <a href=#%e5%85%83%e6%95%b0%e6%8d%ae aria-hidden=true>#</a></h2><p>Hudi 将一系列 <code>CURD</code> 操作称为 <code>Timeline</code>，而 <code>Timeline</code> 中的某次操作称为 <code>Instant</code>，其核心能力就是维护 <code>Timeline</code>，依次提供即时视图以及按到达顺序检索数据。其实现是在元数据目录下以日期进行保存相关信息，如下是一个简单的示例：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>cow/.hoodis/20230523174532904.commit
</span></span><span class=line><span class=cl>cow/.hoodis/20230523174532904.commit.inflight
</span></span><span class=line><span class=cl>cow/.hoodis/20230523174532904.commit.requested
</span></span></code></pre></div><p>其文件格式为 <code>&lt;action timestamp>.&lt;action type>[.&lt;action state>]</code>，介绍如下：</p><ul><li><code>timestamp</code> 首次调度时间，提供毫秒级的粒度，作为单调递增的时间线唯一标识。</li><li><code>action</code> 标示操作类型，包括了 <code>commit/deltacommit</code> 更新数据 (包括插入、更新、删除)；<code>compaction/clean</code> 内部操作；<code>savepoint/restore</code> 恢复操作。</li><li><code>state</code> 标识状态，包括了 A) <code>requested</code> 调度运行；B) <code>inflight</code> 正在运行；C) <code>completed</code> 执行完成。</li></ul><p>元数据通过 <code>JSON</code> 或者 <code>AVRO</code> 格式保存，包括了表的修改信息，可以用来做数据恢复、使用 <code>Snapshot</code> 隔离级别等。其中常见 <code>action</code> 介绍如下：</p><ul><li><code>COMMIT</code> 一批数据的原子写入，COW 写入事务或者 MOR 压缩。</li><li><code>DELTACOMMIT</code> 写入 MOR 表的增量数据，通常可以作为增量日志文件保存。</li><li><code>CLEAN</code> 后台任务，用于清理掉不需要的历史数据。</li><li><code>ROLLBACK</code> 回滚异常的事务。</li><li><code>COMPACTION</code> 后台任务，用于将 MOR 行格式的增量数据合并为列式数据，会生成一个特殊的提交。</li></ul><a class=anchor id=表服务></a><h1>表服务 <a href=#%e8%a1%a8%e6%9c%8d%e5%8a%a1 aria-hidden=true>#</a></h1><p>用于维护 Hudi 表的读写性能、文件数量等，包含多种表服务：</p><ul><li><code>Compaction</code> 合并 <code>BaseFile</code> 和 <code>LogFile</code> 生成新版本文件，提升读取效率。</li><li><code>Clustering</code> 重分布 <code>FileGroup</code>，主要用于合并小文件生成新版本文件，调整数据的分布。</li><li><code>Clean</code> 清理版本过期文件。</li><li><code>Rollback</code> 回滚未完成的 <code>instant</code> 所写入的文件及元数据。</li><li><code>Indexing</code> 构建索引，提升读取性能。</li></ul><a class=anchor id=clean></a><h2>Clean <a href=#clean aria-hidden=true>#</a></h2><p>用于清理不需要的文件。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>----- 是否开启自动清理
</span></span><span class=line><span class=cl>set hoodie.clean.automatic=true;
</span></span><span class=line><span class=cl>----- 是否异步清理
</span></span><span class=line><span class=cl>set hoodie.clean.async=true;
</span></span><span class=line><span class=cl>----- 清理策略 KEEP_LATEST_FILE_VERSIONS KEEP_LATEST_BY_HOURS
</span></span><span class=line><span class=cl>set hoodie.cleaner.policy=KEEP_LATEST_COMMITS;
</span></span><span class=line><span class=cl>----- 最新时间线之前，保留时间线个数，最少保留一个
</span></span><span class=line><span class=cl>set hoodie.cleaner.commits.retained=10;
</span></span></code></pre></div><ul><li><code>KEEP_LATEST_COMMITS</code> 根据提交次数进行清理，默认保留最新 10 个 Commit 所有文件。</li><li><code>KEEP_LATEST_FILE_VERSIONS</code> 保留文件的版本数，默认保留三个版本。</li><li><code>KEEP_LATEST_BY_HOURS</code> 按照小时保留，默认是 24 小时。</li></ul><a class=anchor id=其它></a><h1>其它 <a href=#%e5%85%b6%e5%ae%83 aria-hidden=true>#</a></h1><a class=anchor id=记录级元数据></a><h2>记录级元数据 <a href=#%e8%ae%b0%e5%bd%95%e7%ba%a7%e5%85%83%e6%95%b0%e6%8d%ae aria-hidden=true>#</a></h2><p>通过 Hudi 可以跟踪记录级的时间变化，为了达到此目的，在每个表中同时会维护行相关的元数据，可以查看 <code>.hoodie/hoodie.properties</code> 中的 <code>hoodie.table.create.schema</code> 配置项，主要包含如下几个字段：</p><ul><li><code>_hoodie_record_key</code> 记录主键，用来处理更新和删除。</li><li><code>_hoodie_commit_time</code> 最新记录提交时间。</li><li><code>_hoodie_commit_seqno</code> 提交序列号。</li><li><code>_hoodie_partition_path</code> 分区路径。</li><li><code>_hoodie_file_name</code> 存储记录的文件名。</li></ul><p>可以参考 <a href=https://www.onehouse.ai/blog/hudi-metafields-demystified>Hudi Metafields demystified</a> 中的介绍。</p><a class=anchor id=参考></a><h1>参考 <a href=#%e5%8f%82%e8%80%83 aria-hidden=true>#</a></h1><ul><li><a href=https://hudi.apache.org/tech-specs/>Apache Hudi Technical Specification</a> 官方的技术参考文档。</li><li>官网 <a href=https://hudi.apache.org/>Hudi</a> 提供了下载、文档等，最常用的可以参考 <a href=https://hudi.apache.org/docs/quick-start-guide>Quick Start</a> 中的内容。</li><li><a href=https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-110>Apache Hudi: From Zero To One</a> 比较经典的 Hudi 相关材料。</li></ul></div></div><div class="sidebar col-xl-3 mt-2"><div id=toc class=position-fixed><nav id=TableOfContents><ul><li><a href=#基本概念>基本概念</a><ul><li><a href=#文件布局>文件布局</a></li><li><a href=#表类型>表类型</a></li></ul></li><li><a href=#示例>示例</a><ul><li><a href=#简单介绍>简单介绍</a></li><li><a href=#mergeinto>MergeInto</a></li></ul></li><li><a href=#查询>查询</a></li><li><a href=#目录结构>目录结构</a><ul><li><a href=#元数据>元数据</a></li></ul></li><li><a href=#表服务>表服务</a><ul><li><a href=#clean>Clean</a></li></ul></li><li><a href=#其它>其它</a><ul><li><a href=#记录级元数据>记录级元数据</a></li></ul></li><li><a href=#参考>参考</a></li></ul></nav></div></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class=text-center>Built by GoHalo, generated with <a class=text-muted href=https://gohugo.io>Hugo</a>, and hosted on GitHub Pages</div></div><div class=row><div class=text-center>Copyright © 2013-2025 GoHalo. All Rights Reserved.</div></div></div></footer><script src=https://gohalo.github.io/bootstrap/js/bootstrap.bundle.min.js></script>
<script src=/main.b9cbcb174709877512d64e24f297f66a40c8d91c9a81128cb04bdd7b10247df8.js integrity="sha256-ucvLF0cJh3US1k4k8pf2akDI2RyagRKMsEvdexAkffg=" crossorigin=anonymous></script>
<a href=# class="btn btn-light btn-lg backtop" title=返回顶部><i class="fa fa-angle-double-up" aria-hidden=true></i></a></body></html>