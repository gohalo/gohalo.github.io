<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Spark 使用介绍 | GoHalo</title><link rel=apple-touch-icon sizes=180x180 href=https://gohalo.github.io/favicon/apple-touch-icon.png><link rel=icon href=https://gohalo.github.io/favicon/favicon.ico sizes=any><link rel=icon type=image/png sizes=32x32 href=https://gohalo.github.io/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://gohalo.github.io/favicon/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://gohalo.github.io/favicon/site.webmanifest><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=google-site-verification content="p7jJ5d3kF9yxRhpIo5GgHXAZ1ATKVyZhV2kf6mEGOv0"><meta name=description content="开源的集群运算框架，与 Hadoop 在执行 MapReduce 需要落盘不同，Spark 的数据会尽量在内存中进行计算。
"><link rel=stylesheet href=https://gohalo.github.io/font-awesome/css/font-awesome.min.css><link rel=stylesheet href=/css/syntax.min.c70103877c799b924f50023b6b01eca010d7e2808885a74f9ea662cc47379ae1.css integrity="sha256-xwEDh3x5m5JPUAI7awHsoBDX4oCIhadPnqZizEc3muE=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.c4814ac9dc5fab259f313a787ded4f8e.css integrity="md5-xIFKydxfqyWfMTp4fe1Pjg==" crossorigin=anonymous><style type=text/css>.main p{text-indent:2em}.main li p{text-indent:0}</style><noscript><style>img.lazyload{display:none}</style></noscript></head><body><div class=sticky-top><div class=header-bar></div><nav class="navbar navbar-expand-lg navbar-light bg-light"><div class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=/cn aria-label=GoHalo>GoHalo</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>项目</a><ul class=dropdown-menu aria-labelledby=navbarDropdown><li><a class=dropdown-item href=/cn/project/bastion/>Bastion</a></li><li><a class="dropdown-item disabled" href=/cn/project/bootserver/>BootServer</a></li></ul></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/>博客</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/archives/>归档</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/tags/>标签</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/slide/>幻灯片</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/docs/>文档</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><div class=dropdown><button class="btn dropdown-toggle" id=header-languages data-bs-toggle=dropdown aria-expanded=false data-bs-display=static>
中文</button><ul class="dropdown-menu dropdown-menu-lg-end me-lg-2 shadow rounded border-0" aria-labelledby=header-languages><li><a class=dropdown-item rel=alternate href=https://gohalo.github.io/en hreflang=en lang=en>English</a></li></ul></div><ul class="nav flex-column flex-lg-row"><li class=nav-item><a class="nav-link social-link" href=/cn/about><i class="fa fa-user" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link social-link" href=https://github.com/gohalo title=GitHub><i class="fa fa-github-alt" aria-hidden=true></i></a></li><li class=nav-item><button id=mode class="btn nav-link social-link" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><i class="fa fa-star" aria-hidden=true></i></span>
<span class=toggle-light><i class="fa fa-cog" aria-hidden=true></i></span></button></li></ul></div></div></nav></div><div class="main container-xxl" role=document><div class="blog row"><div class="col-md-12 col-xl-9 mt-4"><div class=header><h1>Spark 使用介绍</h1><div class="meta mb-3"><i class="fa fa-calendar" aria-hidden=true></i>
<span class=mx-2>2022-09-19</span>
<i class="fa fa-tags" aria-hidden=true></i>
<a class=text-body href=https://gohalo.github.io/cn/tags/warehouse/ role=button>warehouse</a></div></div><hr><div class=content><p>开源的集群运算框架，与 Hadoop 在执行 MapReduce 需要落盘不同，Spark 的数据会尽量在内存中进行计算。</p><a class=anchor id=简介></a><h1>简介 <a href=#%e7%ae%80%e4%bb%8b aria-hidden=true>#</a></h1><a class=anchor id=安装部署></a><h2>安装部署 <a href=#%e5%ae%89%e8%a3%85%e9%83%a8%e7%bd%b2 aria-hidden=true>#</a></h2><p>与 Spark 相关的包可以从官网或者 Apache 的归档仓库 <a href=http://archive.apache.org/dist/spark/>archive.apache.org</a> 中下载相关的包，注意其中的 hadoop 版本。
官方有点慢，可以从国内镜像 <a href=https://mirrors.aliyun.com/apache/spark/>mirrors.aliyun.com</a> 中下载。</p><p>这里以 Without Hadoop 的为例，如果只使用 Spark 那么建议使用 With Hadoop 版本，否则 SparkSQL 可能会无法使用，需要单独下载缺失的 JAR 包或者指定 Hadoop 环境才可以。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>----- 增加如下全局环境变量
</span></span><span class=line><span class=cl># vim /etc/profile
</span></span><span class=line><span class=cl>export SPARK_HOME=/opt/warehouse/spark
</span></span><span class=line><span class=cl>export PATH=$PATH:$SPARK_HOME/bin
</span></span><span class=line><span class=cl>----- 增加Spark环境变量，这里使用的是WithoutHadoop版本
</span></span><span class=line><span class=cl># vim conf/spark-env.sh
</span></span><span class=line><span class=cl>export SPARK_DIST_CLASSPATH=$(/opt/module/hadoop/bin/hadoop classpath)
</span></span><span class=line><span class=cl>export SPARK_MASTER_HOST=localhost
</span></span><span class=line><span class=cl>export SPARK_MASTER_PORT=7077
</span></span></code></pre></div><a class=anchor id=单节点运行></a><h2>单节点运行 <a href=#%e5%8d%95%e8%8a%82%e7%82%b9%e8%bf%90%e8%a1%8c aria-hidden=true>#</a></h2><p>有多种方式可以运行 Spark 环境，先介绍最简单的。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>----- 本地运行，用于测试以及基本功能验证，如下会启动两个线程。
</span></span><span class=line><span class=cl>spark-sql --master local[2]
</span></span><span class=line><span class=cl>spark-shell --master local[2]
</span></span></code></pre></div><p>也可以本地启动。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>----- 启动/停止服务，默认会本地启动一个Worker以及Master，可以通过jps命令查看
</span></span><span class=line><span class=cl># sbin/start-all.sh
</span></span><span class=line><span class=cl># sbin/stop-all.sh
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>----- 启动shell交互，可以显示指定，如下是默认值
</span></span><span class=line><span class=cl>spark-sql --master spark://127.0.0.1:7077
</span></span><span class=line><span class=cl>spark-shell --master spark://127.0.0.1:7077
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>----- 执行一个简单示例，计算Pi值，
</span></span><span class=line><span class=cl># bin/run-example SparkPi
</span></span><span class=line><span class=cl># bin/spark-submit examples/src/main/python/pi.py
</span></span><span class=line><span class=cl># bin/spark-submit --master spark://127.0.0.1:7077 \
</span></span><span class=line><span class=cl>    --class org.apache.spark.examples.SparkPi      \
</span></span><span class=line><span class=cl>    examples/jars/spark-examples_2.12-3.4.1.jar 10
</span></span><span class=line><span class=cl>----- 也可以使用Yarn做调度
</span></span><span class=line><span class=cl># spark-shell --master yarn --deploy-mode client
</span></span></code></pre></div><p>注意，在启动 Worker 节点时，会通过 <code>ssh</code> 远程执行 (即使是本地也一样)，所以，需要确保 <code>ssh</code> 是可以连接 Worker 节点的，默认是 <code>localhost</code> 本地，可以通过 <code>ssh localhost true</code> 进行验证，主要不报错就说明正常。</p><p>另外，当执行 SQL 时，包括在 <code>spark-sql</code> 中或者通过 <code>spark.sql(...)</code> 执行，如果没有配置如下的 Hive 集群，默认会在当前目录下创建 <code>metastore_db</code> 目录保存元数据。</p><a class=anchor id=hive-集成></a><h2>Hive 集成 <a href=#hive-%e9%9b%86%e6%88%90 aria-hidden=true>#</a></h2><p>也就是将 Hive 作为元数据管理，可以用来维护相关的数据组织信息，可以通过如下方式配置。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>---- 复制Hive的配置文件以及元数据驱动JAR包，也可以启动时通过 --jars 参数指定
</span></span><span class=line><span class=cl># cp hive/conf/hive-site.xml spark/conf/
</span></span><span class=line><span class=cl># cp hive/lib/mysql-connector-java-8.0.28.jar spark/jars/
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>---- 然后通过如下方式执行
</span></span><span class=line><span class=cl># spark-shell --master local[2]
</span></span><span class=line><span class=cl>scala&gt; spark.sql(&#34;show databases&#34;).show()
</span></span><span class=line><span class=cl>scala&gt; spark.sql(&#34;select * from user_info&#34;).show()
</span></span><span class=line><span class=cl># spark-sql --master local[2]
</span></span><span class=line><span class=cl>spark-sql (default)&gt; show databases;
</span></span><span class=line><span class=cl>spark-sql (default)&gt; select * from user_info;
</span></span></code></pre></div><a class=anchor id=简介-1></a><h1>简介 <a href=#%e7%ae%80%e4%bb%8b-1 aria-hidden=true>#</a></h1><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>lines</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=o>(</span><span class=s>&#34;hdfs://...&#34;</span><span class=o>)</span>   <span class=c1>// HadoopRDD ==map==&gt; MapPartitionsRDD
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>lines</span><span class=o>.</span><span class=n>flatMap</span><span class=o>(</span><span class=k>_</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>))</span>   <span class=c1>// MapPartitionsRDD
</span></span></span><span class=line><span class=cl><span class=c1></span>     <span class=o>.</span><span class=n>filter</span><span class=o>(</span><span class=k>_</span><span class=o>.</span><span class=n>length</span> <span class=o>&gt;=</span> <span class=mi>2</span><span class=o>)</span>   <span class=c1>// MapPartitionsRDD
</span></span></span><span class=line><span class=cl><span class=c1></span>     <span class=o>.</span><span class=n>map</span><span class=o>((</span><span class=k>_</span><span class=o>,</span> <span class=mi>1</span><span class=o>))</span>             <span class=c1>// MapPartitionsRDD
</span></span></span><span class=line><span class=cl><span class=c1></span>     <span class=o>.</span><span class=n>reduceByKey</span><span class=o>(</span><span class=k>_</span> <span class=o>+</span> <span class=k>_</span><span class=o>)</span>      <span class=c1>// ShuffleRDD
</span></span></span></code></pre></div><a class=anchor id=任务拆分></a><h2>任务拆分 <a href=#%e4%bb%bb%e5%8a%a1%e6%8b%86%e5%88%86 aria-hidden=true>#</a></h2><p>Spark 是由一堆的转换算子组成，会划分为 Job Stage Task 来执行。</p><ul><li>根据算子是否为 Action 算子来划分 Job 。</li><li>根据算子运算是否需要 Shuffle 来划分 Stage 。</li><li>根据 RDD 分区数来划分 Task 。</li></ul><p>例如如下的代码。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=n>rdd</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=na>parallelize</span><span class=o>([</span><span class=mi>1</span><span class=o>,</span> <span class=mi>2</span><span class=o>,</span> <span class=mi>3</span><span class=o>],</span> <span class=mi>3</span><span class=o>)</span>         <span class=c1>// 初始化三个分区RDD
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>rdd</span><span class=o>.</span><span class=na>distinct</span><span class=o>().</span><span class=na>collect</span><span class=o>()</span>                   <span class=c1>// 转换算子+Action算子
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>rdd</span><span class=o>.</span><span class=na>filter</span><span class=o>(</span><span class=n>lambda</span> <span class=n>x</span><span class=o>:</span><span class=n>x</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>!=</span> <span class=mi>0</span><span class=o>).</span><span class=na>collect</span><span class=o>()</span>  <span class=c1>// 转换算子+Action算子
</span></span></span></code></pre></div><p>上述包含了两个 Action 算子，也就是总计两个 Job：</p><ol><li><code>distinct</code> 含 Shuffle 操作，算一个 Stage，而 Collect 也算，总计两个 Stage，而且 RDD 含三个分区，总计有 <code>2x3=6</code> 个 Task 。</li><li><code>filter</code> 不含 Shuffle 操作，不算一个 Stage，总计一个 Stage，总计有 <code>1x3=3</code> 个 Task 。</li></ol><p>总计有 2 个 Job，3 个 Stage，9 个 Task 。</p><a class=anchor id=shuffle></a><h2>Shuffle <a href=#shuffle aria-hidden=true>#</a></h2><p>在 Spark 1.1 之前采用的是 Hash Shuffle 机制，在 1.1 版本引入了基于 Sort 的 Shuffle 机制，并从 1.2 版本之后作为默认，而 2.0 则不再使用 Hash 了。</p><p>其中 Stage 会根据是否有 Shuffle 进行划分。</p><a class=anchor id=rdd></a><h1>RDD <a href=#rdd aria-hidden=true>#</a></h1><p>Resilient Distributed Datasets, RDD 是 Spark 的核心数据结构，所有数据计算操作均基于该结构实现，</p><p>这里最关键的就是 <code>compute()</code> 接口实现，定义了如何从特定分区中获取数据，用来实现具体的计算逻辑，是每个 Task 执行的起点。</p><a class=anchor id=常见问题></a><h1>常见问题 <a href=#%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98 aria-hidden=true>#</a></h1><a class=anchor id=winutilsexe></a><h2>winutils.exe <a href=#winutilsexe aria-hidden=true>#</a></h2><p>全部的报错内容为 <code>java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.</code> ，主要是 Windows 环境上的依赖问题。</p><p>hadoop 本身是基于 Linux 环境开发，不能直接部署运行到 Windows 上，如果需要在本地运行类似 Spark 的任务，那么需要通过 <code>winutils.exe</code> <code>hadoop.dll</code> 来模拟，可以直接从 <a href=https://github.com/cdarlint/winutils>Github</a> 下载，需要注意对应的版本。</p><p>然后在环境变量中设置 <code>HADOOP_HOME</code> ，并将 <code>%HADOOP_HOME%\bin</code> 添加到环境变量 <code>PATH</code> 中。</p><p>注意，使用 IDEA 时，直接修改系统的环境变量是无法立即生效的，需要在运行配置中的 <code>Environment variables</code> 内添加，然后在代码中就可以通过 <code>System.out.println(System.getenv("HADOOP_HOME"));</code> 确定是否生效。</p></div></div><div class="sidebar col-xl-3 mt-2"><div id=toc class=position-fixed><nav id=TableOfContents><ul><li><a href=#简介>简介</a><ul><li><a href=#安装部署>安装部署</a></li><li><a href=#单节点运行>单节点运行</a></li><li><a href=#hive-集成>Hive 集成</a></li></ul></li><li><a href=#简介-1>简介</a><ul><li><a href=#任务拆分>任务拆分</a></li><li><a href=#shuffle>Shuffle</a></li></ul></li><li><a href=#rdd>RDD</a></li><li><a href=#常见问题>常见问题</a><ul><li><a href=#winutilsexe>winutils.exe</a></li></ul></li></ul></nav></div></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class=text-center>Built by GoHalo, generated with <a class=text-muted href=https://gohugo.io>Hugo</a>, and hosted on GitHub Pages</div></div><div class=row><div class=text-center>Copyright © 2013-2025 GoHalo. All Rights Reserved.</div></div></div></footer><script src=https://gohalo.github.io/bootstrap/js/bootstrap.bundle.min.js></script>
<script src=/main.b9cbcb174709877512d64e24f297f66a40c8d91c9a81128cb04bdd7b10247df8.js integrity="sha256-ucvLF0cJh3US1k4k8pf2akDI2RyagRKMsEvdexAkffg=" crossorigin=anonymous></script>
<a href=# class="btn btn-light btn-lg backtop" title=返回顶部><i class="fa fa-angle-double-up" aria-hidden=true></i></a></body></html>