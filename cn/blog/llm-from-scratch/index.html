<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>LLM 使用简介 | GoHalo</title><link rel=apple-touch-icon sizes=180x180 href=https://gohalo.github.io/favicon/apple-touch-icon.png><link rel=icon href=https://gohalo.github.io/favicon/favicon.ico sizes=any><link rel=icon type=image/png sizes=32x32 href=https://gohalo.github.io/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://gohalo.github.io/favicon/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://gohalo.github.io/favicon/site.webmanifest><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=google-site-verification content="p7jJ5d3kF9yxRhpIo5GgHXAZ1ATKVyZhV2kf6mEGOv0"><meta name=description content="主要是 Karpathy 的相关实现，通过简单的示例、资源介绍 LLM 相关的概念。
"><link rel=stylesheet href=https://gohalo.github.io/font-awesome/css/font-awesome.min.css><link rel=stylesheet href=/css/syntax.min.c70103877c799b924f50023b6b01eca010d7e2808885a74f9ea662cc47379ae1.css integrity="sha256-xwEDh3x5m5JPUAI7awHsoBDX4oCIhadPnqZizEc3muE=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.c4814ac9dc5fab259f313a787ded4f8e.css integrity="md5-xIFKydxfqyWfMTp4fe1Pjg==" crossorigin=anonymous><style type=text/css>.main p{text-indent:2em}.main li p{text-indent:0}</style><noscript><style>img.lazyload{display:none}</style></noscript></head><body><div class=sticky-top><div class=header-bar></div><nav class="navbar navbar-expand-lg navbar-light bg-light"><div class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=/cn aria-label=GoHalo>GoHalo</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>项目</a><ul class=dropdown-menu aria-labelledby=navbarDropdown><li><a class=dropdown-item href=/cn/project/bastion/>Bastion</a></li><li><a class="dropdown-item disabled" href=/cn/project/bootserver/>BootServer</a></li></ul></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/>博客</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/archives/>归档</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/blog/tags/>标签</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/slide/>幻灯片</a></li><li class=nav-item><a class=nav-link aria-current=page href=/cn/docs/>文档</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><div class=dropdown><button class="btn dropdown-toggle" id=header-languages data-bs-toggle=dropdown aria-expanded=false data-bs-display=static>
中文</button><ul class="dropdown-menu dropdown-menu-lg-end me-lg-2 shadow rounded border-0" aria-labelledby=header-languages><li><a class=dropdown-item rel=alternate href=https://gohalo.github.io/en hreflang=en lang=en>English</a></li></ul></div><ul class="nav flex-column flex-lg-row"><li class=nav-item><a class="nav-link social-link" href=/cn/about><i class="fa fa-user" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link social-link" href=https://github.com/gohalo title=GitHub><i class="fa fa-github-alt" aria-hidden=true></i></a></li><li class=nav-item><button id=mode class="btn nav-link social-link" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><i class="fa fa-star" aria-hidden=true></i></span>
<span class=toggle-light><i class="fa fa-cog" aria-hidden=true></i></span></button></li></ul></div></div></nav></div><div class="main container-xxl" role=document><div class="blog row"><div class="col-md-12 col-xl-9 mt-4"><div class=header><h1>LLM 使用简介</h1><div class="meta mb-3"><i class="fa fa-calendar" aria-hidden=true></i>
<span class=mx-2>2023-12-08</span>
<i class="fa fa-tags" aria-hidden=true></i>
<a class=text-body href=https://gohalo.github.io/cn/tags/ai/ role=button>ai</a>
<a class=text-body href=https://gohalo.github.io/cn/tags/linux/ role=button>linux</a></div></div><hr><div class=content><p>主要是 Karpathy 的相关实现，通过简单的示例、资源介绍 LLM 相关的概念。</p><a class=anchor id=简介></a><h1>简介 <a href=#%e7%ae%80%e4%bb%8b aria-hidden=true>#</a></h1><p>大多数的 LLM 都采用了类似 GPT 的架构，基于 Transformer 由解码器组成的网络结构，采用自回归方式构建，只是在位置编码、归一化位置、激活函数等细节上各有不同。</p><p>当前大部分 LLM 模型都只使用了 Decoder 模块，相比 Encoder 来说，就是在计算 <code>Q*K</code> 时引入了 Mask 以确保当前位置只能关注前面已经生成的内容。</p><p>原始的 <a href=https://github.com/openai/gpt-2>GPT2</a> 包含了最基础的实现，通过 TenserFlow 开发，同时包含了类似分词器的能力。</p><p>这里基本都是 Karpathy 仓库的实现介绍，基于 <a href=https://arxiv.org/abs/2305.07759>TinyStories</a> 数据集，原始数据可以从 <a href=https://huggingface.co/datasets/roneneldan/TinyStories>HuggingFace</a> 下载。</p><p>评估一个大模型时，会有如下的参数：</p><ul><li><code>Parameters</code> 参数数量，一般是以 Billion, B 十亿为单位，可以简单理解为模型神经元的数量，数量越多模型的处理信息能力越强，对数据中复杂关系的把握也越精准，同时训练和推理的成本也越高。</li></ul><a class=anchor id=分词></a><h2>分词 <a href=#%e5%88%86%e8%af%8d aria-hidden=true>#</a></h2><p>在 <code>HuggingFace</code> 的 <code>Transformer</code> 包中包含了 <code>AutoTokenizer.from_pretrained()</code> 的实现，用于加载预训练的文本处理模型，用于将文本数据转换为可接受的输入格式，如参可以是类似 <code>gpt2</code> 这种通用模型，也可以从指定目录加载。</p><p>其中 <a href=https://tiktokenizer.vercel.app>tiktokenizer.vercel.app</a> 包含了不同模型的分词可视化。</p><a class=anchor id=其它></a><h2>其它 <a href=#%e5%85%b6%e5%ae%83 aria-hidden=true>#</a></h2><a class=anchor id=代理设置></a><h3>代理设置 <a href=#%e4%bb%a3%e7%90%86%e8%ae%be%e7%bd%ae aria-hidden=true>#</a></h3><p>很多可能需要通过 <code>git clone</code> 下载预训练模型，国内可能会被墙，可以使用 <a href=https://hf-mirror.com>hf-mirror.com</a> 代理，除了常规页面上的使用方式，在通过 <code>git</code> 下载时直接将 <code>huggingface.com</code> 替换为 <code>hf-mirror.com</code> 即可。</p><a class=anchor id=llama2c></a><h1>llama2.c <a href=#llama2c aria-hidden=true>#</a></h1><p>简单的 <a href=https://github.com/karpathy/llama2.c>llama2.c</a> 模型，通过 <code>Llama2</code> 架构进行训练、推理，其中训练通过 PyTorch 实现，而推理则使用最简单的 C 实现，而且提供了脚本来转换 llama2 参数，</p><a class=anchor id=示例></a><h2>示例 <a href=#%e7%a4%ba%e4%be%8b aria-hidden=true>#</a></h2><p>包括了推理以及训练阶段。</p><a class=anchor id=推理></a><h3>推理 <a href=#%e6%8e%a8%e7%90%86 aria-hidden=true>#</a></h3><p>按照文档的介绍，直接从 HuggingFace 上下载已经训练好的参数模型，包括了常用的 <a href=https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin>stories15M</a> 和 <a href=https://huggingface.co/karpathy/tinyllamas/resolve/main/stories42M.bin>stories42M</a> 两个，然后通过如下命令简单运行。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>make run
</span></span><span class=line><span class=cl>./run stories15M.bin
</span></span></code></pre></div><p>其中常用的参数有：</p><ul><li><code>-t 1.0</code> 热度 (Temperature) 用来控制语言模型输出的随机度，高热度生成更难预料且富有创造性的结果，而第热度则更保守。</li><li><code>-p 0.9</code> Top-p 核心采样 (Nucleus Sampling) 累积阈值超过该参数的最佳词汇，模型会从这组词汇中随即抽取以生成输出。</li><li><code>-i "Your Prompt"</code> 不同的提示语，会生成不同的文档内容。</li></ul><a class=anchor id=训练></a><h3>训练 <a href=#%e8%ae%ad%e7%bb%83 aria-hidden=true>#</a></h3><p>上述的模型参数国内可能无法下载，可以自己训练，主要是通过 Python 实现，国内可以直接搜索 <code>TinyStories_all_data.tar.gz</code> 关键字，有很多仓库可以下载。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>----- 会从 HuggingFace 上下载并解压 TinyStories 数据集
</span></span><span class=line><span class=cl>python tinystories.py download
</span></span><span class=line><span class=cl>----- 进行分词
</span></span><span class=line><span class=cl>python tinystories.py pretokenize
</span></span><span class=line><span class=cl>----- 开始训练
</span></span><span class=line><span class=cl>python train.py
</span></span></code></pre></div><a class=anchor id=源码解析></a><h2>源码解析 <a href=#%e6%ba%90%e7%a0%81%e8%a7%a3%e6%9e%90 aria-hidden=true>#</a></h2><p>其推理阶段很简单，会循环调用 <code>forward()</code> <code>sample()</code> <code>decode()</code> 生成，其中核心在 <code>forward()</code> 阶段。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>main()
</span></span><span class=line><span class=cl> |-build_transformer()
</span></span><span class=line><span class=cl> | |-read_checkpoint() 读取配置、参数信息
</span></span><span class=line><span class=cl> | | |-memory_map_weights() 关键权重信息，这里包含了相关参数
</span></span><span class=line><span class=cl> | |-malloc_run_state()
</span></span><span class=line><span class=cl> |-build_tokenizer() 使用 Byte Pair Encoding, BPE
</span></span><span class=line><span class=cl> |-build_sampler()
</span></span><span class=line><span class=cl> |======&gt; 上面基本构建了整个模型，参数采用 float 类型，如下根据具体场景调用
</span></span><span class=line><span class=cl> |-generate()
</span></span><span class=line><span class=cl> | |-encode() 对输入进行编码
</span></span><span class=line><span class=cl> | |====&gt; 这里开始循环执行 Step 次
</span></span><span class=line><span class=cl> | |-forward()
</span></span><span class=line><span class=cl> | |-sample() 超过输入 Token 之后开始采样，如果是 BOS 则结束
</span></span><span class=line><span class=cl> | |-decode() 输出文本
</span></span><span class=line><span class=cl> |-chat()
</span></span></code></pre></div><p>如下所谓的 <code>llama2</code> 模型指的是 <code>7B</code> 参数规模。</p><p>通常隐藏层的维度要比输入层的维度要高。</p><p>其中核心的函数包括了：</p><ul><li><code>rmsnorm</code> 计算均方根标准化。</li><li><code>matmul</code> 矩阵乘法。</li><li><code>softmax</code></li></ul><p>其中的 <code>seq_len</code> 决定了在训练过程中生成文本的最大有效长度，在推理时，目前的策略是始终小于训练长度。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=k>typedef</span> <span class=k>struct</span> <span class=p>{</span>    <span class=c1>//                                   Tiny      7B   Mistral 7B
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>dim</span><span class=p>;</span>        <span class=c1>// 词向量维度，输入维度         8     288    4096         4096
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>hidden_dim</span><span class=p>;</span> <span class=c1>// 隐藏层(FFN 前向网络)维度    24     768                14336 3.5*dim
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>n_layers</span><span class=p>;</span>   <span class=c1>// 层数                                 6      32           32
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>n_heads</span><span class=p>;</span>    <span class=c1>// number of query heads        4       6                   32
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>n_kv_heads</span><span class=p>;</span> <span class=c1>// number of key/value heads    2       6                    8
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>vocab_size</span><span class=p>;</span> <span class=c1>// 词表大小，英文通常是             32000                32000
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>seq_len</span><span class=p>;</span>    <span class=c1>// max sequence length                256
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span> <span class=n>Config</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=c1>// head_dim = dim / n_heads    每个头的维度                                 128
</span></span></span><span class=line><span class=cl><span class=c1>// window_size                                                             4096
</span></span></span><span class=line><span class=cl><span class=c1>// context_len                                                             8192
</span></span></span><span class=line><span class=cl><span class=c1>// 如下是计算过程中可能使用的变量
</span></span></span><span class=line><span class=cl><span class=c1>// kv_dim=dim*n_kv_heads/n_heads                    2     288              8192
</span></span></span><span class=line><span class=cl><span class=c1>// head_size=dim/n_heads                            2      48             8192
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=k>typedef</span> <span class=k>struct</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// token embedding table
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>token_embedding_table</span><span class=p>;</span>    <span class=c1>// (vocab_size, dim)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// weights for rmsnorms
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>rms_att_weight</span><span class=p>;</span> <span class=c1>// (layer, dim) RMSNorm 权重
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>rms_ffn_weight</span><span class=p>;</span> <span class=c1>// (layer, dim)
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=c1>// weights for matmuls. note dim == n_heads * head_size
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>wq</span><span class=p>;</span> <span class=c1>// (layer, dim, n_heads * head_size)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>wk</span><span class=p>;</span> <span class=c1>// (layer, dim, n_kv_heads * head_size)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>wv</span><span class=p>;</span> <span class=c1>// (layer, dim, n_kv_heads * head_size)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>wo</span><span class=p>;</span> <span class=c1>// (layer, n_heads * head_size, dim)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// weights for ffn
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>w1</span><span class=p>;</span> <span class=c1>// (layer, hidden_dim, dim)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>w2</span><span class=p>;</span> <span class=c1>// (layer, dim, hidden_dim)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>w3</span><span class=p>;</span> <span class=c1>// (layer, hidden_dim, dim)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// final rmsnorm
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>rms_final_weight</span><span class=p>;</span> <span class=c1>// (dim,)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// (optional) classifier weights for the logits, on the last layer
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>wcls</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=n>TransformerWeights</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>typedef</span> <span class=k>struct</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=o>*</span><span class=n>x</span><span class=p>;</span> <span class=c1>// activation at current time stamp (dim,)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span> <span class=o>*</span><span class=n>xb</span><span class=p>;</span> <span class=c1>// same, but inside a residual branch (dim,)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span> <span class=o>*</span><span class=n>xb2</span><span class=p>;</span> <span class=c1>// an additional buffer just for convenience (dim,)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span> <span class=o>*</span><span class=n>hb</span><span class=p>;</span> <span class=c1>// buffer for hidden dimension in the ffn (hidden_dim,)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span> <span class=o>*</span><span class=n>hb2</span><span class=p>;</span> <span class=c1>// buffer for hidden dimension in the ffn (hidden_dim,)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span> <span class=o>*</span><span class=n>q</span><span class=p>;</span> <span class=c1>// query (dim,)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span> <span class=o>*</span><span class=n>k</span><span class=p>;</span> <span class=c1>// key (dim,)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span> <span class=o>*</span><span class=n>v</span><span class=p>;</span> <span class=c1>// value (dim,)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span> <span class=o>*</span><span class=n>att</span><span class=p>;</span> <span class=c1>// buffer for scores/attention values (n_heads, seq_len)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span> <span class=o>*</span><span class=n>logits</span><span class=p>;</span> <span class=c1>// output logits
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// kv cache
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>key_cache</span><span class=p>;</span>   <span class=c1>// (layer, seq_len, dim)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>value_cache</span><span class=p>;</span> <span class=c1>// (layer, seq_len, dim)
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span> <span class=n>RunState</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>typedef</span> <span class=k>struct</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>Config</span> <span class=n>config</span><span class=p>;</span>              <span class=c1>// 保存了神经网络整体结构
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>TransformerWeights</span> <span class=n>weights</span><span class=p>;</span> <span class=c1>// 模型训练的结果
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>RunState</span> <span class=n>state</span><span class=p>;</span>             <span class=c1>// 保存的中间状态
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>fd</span><span class=p>;</span> <span class=c1>// file descriptor for memory mapping
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span><span class=o>*</span> <span class=n>data</span><span class=p>;</span> <span class=c1>// memory mapped data pointer
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>ssize_t</span> <span class=n>file_size</span><span class=p>;</span> <span class=c1>// size of the checkpoint file in bytes
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span> <span class=n>Transformer</span><span class=p>;</span>
</span></span></code></pre></div><a class=anchor id=llmc></a><h1>llm.c <a href=#llmc aria-hidden=true>#</a></h1><p>相同作者的 <a href=https://github.com/karpathy/llm.c>llm.c</a> 实现，相比之前的 nanoGPT 要更加简单，用来训练模型，实现相当简单，只有 1K 左右的 C 代码。</p><a class=anchor id=示例-1></a><h2>示例 <a href=#%e7%a4%ba%e4%be%8b-1 aria-hidden=true>#</a></h2><p>提供了 <code>Shakespeare</code> 和 <code>TinyStories</code> 两个测试数据集 ，会首先尝试前者。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>----- 下载一些已经提前准备好的数据集，或者进行预处理和分词
</span></span><span class=line><span class=cl>bash dev/download_starter_pack.sh
</span></span><span class=line><span class=cl>----- 生成 dev/data/tinystories/TinyStories_{train/val}.bin 文件
</span></span><span class=line><span class=cl>python dev/data/tinystories.py
</span></span><span class=line><span class=cl>----- 生成 dev/data/tinyshakespeare/tiny_shakespeare_{train/val}.bin 文件
</span></span><span class=line><span class=cl>python dev/data/tinyshakespeare.py
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>----- 然后编译训练
</span></span><span class=line><span class=cl>make train_gpt2
</span></span><span class=line><span class=cl>OMP_NUM_THREADS=8 ./train_gpt2
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>----- 用来测试 C 和 PyTorch 结果相同
</span></span><span class=line><span class=cl>make test_gpt2
</span></span><span class=line><span class=cl>./test_gpt2
</span></span></code></pre></div><a class=anchor id=源码解析-1></a><h2>源码解析 <a href=#%e6%ba%90%e7%a0%81%e8%a7%a3%e6%9e%90-1 aria-hidden=true>#</a></h2><a class=anchor id=nanogpt></a><h1>nanoGPT <a href=#nanogpt aria-hidden=true>#</a></h1><p>纯 Python 实现。</p><ul><li><code>vocab_size</code> 词汇量大小</li><li><code>context_length</code> 上下文长度</li><li><code>emb_dim</code> 输入 Token 转换为向量大小</li><li><code>n_heads</code> 多头输入大小</li></ul><p>常用简写。</p><ul><li><code>Begin Of Sentence, BOS</code> 句子开始；<code>End Of Sentence, EOS</code> 句子结束。</li></ul><a class=anchor id=并行加速></a><h1>并行加速 <a href=#%e5%b9%b6%e8%a1%8c%e5%8a%a0%e9%80%9f aria-hidden=true>#</a></h1><p>Checkpoint 是降低 LLM 训练成本的关键技术，可以在失败后继续而非从头开始，而且可以在不同的阶段评估模型性能。</p><p>并行加速包括了数据并行 (Data Parallelism)、模型并行 (Model Parallelism)、流水线并行 (Pipeline Parallelism)。</p><a class=anchor id=其它-1></a><h1>其它 <a href=#%e5%85%b6%e5%ae%83-1 aria-hidden=true>#</a></h1><ul><li><a href=https://github.com/openai/tiktoken>tiktoken</a> 由 OpenAI 开源的 Python 库，实现了 Byte Pair Encoding, BPE 算法，并对性能作了极大的优化。</li></ul><a class=anchor id=残差连接></a><h2>残差连接 <a href=#%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5 aria-hidden=true>#</a></h2><p>传统神经网络中，每一层的输出是对前一层的输出进行变换得到，而残差连接 (Residual Connection) 是将前一层的输出与后一层的输出相加得到，主要是为了解决深层网络中信息衰减和梯度消失的问题。</p><a class=anchor id=rlhf></a><h2>RLHF <a href=#rlhf aria-hidden=true>#</a></h2><p>GPT 的整个训练过程分成了三阶段，Base模型、微调 (SFT) 模型、RLHF 模型。</p><a class=anchor id=rope></a><h2>RoPE <a href=#rope aria-hidden=true>#</a></h2><p>在 LLM 中词出现的位置是很关键的因素，不同位置表达的语义可能天差地别，常规的包含了绝对位置编码和相对位置编码。另外，使用绝对位置编码时，可能会出现训练和预测 Token 长度不一致导致效果变差。</p><p>RoPE 实际就是选择某种计算方式，以通过绝对位置编码来表征相对位置编码。</p><a class=anchor id=gguf></a><h2>GGUF <a href=#gguf aria-hidden=true>#</a></h2><p>用于存储大模型预训练结果，相比 HuggingFace 和 Torch 的文件格式，提供了更高效的数据存储和访问格式。</p><a class=anchor id=参考></a><h1>参考 <a href=#%e5%8f%82%e8%80%83 aria-hidden=true>#</a></h1><ul><li><a href=https://karpathy.ai/zero-to-hero.html>Zero To Hero</a> 由 Karpathy 录制的相当不错的入门视频，极力推荐。</li><li><a href=https://www.llama.com>llama.com</a> 官方网站，还可以参考 <a href=https://github.com/LlamaFamily/Llama-Chinese>中文社区</a>、<a href=https://github.com/ymcui/Chinese-LLaMA-Alpaca>Alpaca</a> 以及 <a href=https://github.com/meta-llama/llama-cookbook>Cookbook</a> 含很不错的介绍，模型部署、微调方法、RAG 等等。</li></ul></div></div><div class="sidebar col-xl-3 mt-2"><div id=toc class=position-fixed><nav id=TableOfContents><ul><li><a href=#简介>简介</a><ul><li><a href=#分词>分词</a></li><li><a href=#其它>其它</a></li></ul></li><li><a href=#llama2c>llama2.c</a><ul><li><a href=#示例>示例</a></li><li><a href=#源码解析>源码解析</a></li></ul></li><li><a href=#llmc>llm.c</a><ul><li><a href=#示例-1>示例</a></li><li><a href=#源码解析-1>源码解析</a></li></ul></li><li><a href=#nanogpt>nanoGPT</a></li><li><a href=#并行加速>并行加速</a></li><li><a href=#其它-1>其它</a><ul><li><a href=#残差连接>残差连接</a></li><li><a href=#rlhf>RLHF</a></li><li><a href=#rope>RoPE</a></li><li><a href=#gguf>GGUF</a></li></ul></li><li><a href=#参考>参考</a></li></ul></nav></div></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class=text-center>Built by GoHalo, generated with <a class=text-muted href=https://gohugo.io>Hugo</a>, and hosted on GitHub Pages</div></div><div class=row><div class=text-center>Copyright © 2013-2025 GoHalo. All Rights Reserved.</div></div></div></footer><script src=https://gohalo.github.io/bootstrap/js/bootstrap.bundle.min.js></script>
<script src=/main.b9cbcb174709877512d64e24f297f66a40c8d91c9a81128cb04bdd7b10247df8.js integrity="sha256-ucvLF0cJh3US1k4k8pf2akDI2RyagRKMsEvdexAkffg=" crossorigin=anonymous></script>
<a href=# class="btn btn-light btn-lg backtop" title=返回顶部><i class="fa fa-angle-double-up" aria-hidden=true></i></a></body></html>